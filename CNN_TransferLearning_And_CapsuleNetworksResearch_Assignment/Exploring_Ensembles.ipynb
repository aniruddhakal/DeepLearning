{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import h5py\n",
    "\n",
    "\n",
    "def plot_graph(history, n_epochs, save_flag, target_file):\n",
    "    \"\"\"\n",
    "    this plot code is copied from Week 9 lecture slides of 'Deep Learning'\n",
    "    \"\"\"\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure(dpi=300)\n",
    "    plt.plot(np.arange(0, n_epochs), history.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, n_epochs), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(np.arange(0, n_epochs), history.history[\"acc\"], label=\"train_acc\")\n",
    "    plt.plot(np.arange(0, n_epochs), history.history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    if not save_flag:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(target_file, dpi=300)\n",
    "\n",
    "\n",
    "def loadDataH5(h5_file='./data1.h5'):\n",
    "    with h5py.File(h5_file, 'r') as hf:\n",
    "        trainX = np.array(hf.get('trainX'))\n",
    "        trainY = np.array(hf.get('trainY'))\n",
    "        valX = np.array(hf.get('valX'))\n",
    "        valY = np.array(hf.get('valY'))\n",
    "        print(trainX.shape, trainY.shape)\n",
    "        print(valX.shape, valY.shape)\n",
    "\n",
    "    return trainX, trainY, valX, valY\n",
    "\n",
    "\n",
    "def get_default_model_part1(input_shape, n_classes, filter_counts, INCLUDE_DROPOUT):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filter_counts[0], (5, 5), input_shape=input_shape,\n",
    "                                     activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    INCLUDE_DROPOUT and model.add(tf.keras.layers.SpatialDropout2D(rate=0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_config1_model_part1(input_shape, n_classes, filter_counts, INCLUDE_DROPOUT):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filter_counts[0], (5, 5), input_shape=input_shape,\n",
    "                                     activation='relu', padding='same'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    INCLUDE_DROPOUT and model.add(tf.keras.layers.SpatialDropout2D(0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filter_counts[1], (3, 3), padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    INCLUDE_DROPOUT and model.add(tf.keras.layers.SpatialDropout2D(0.15))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(500, activation='relu'))\n",
    "    INCLUDE_DROPOUT and model.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_config2_model_part1(input_shape, n_classes, filter_counts, INCLUDE_DROPOUT):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filter_counts[0], (5, 5), input_shape=input_shape,\n",
    "                                     activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    INCLUDE_DROPOUT and model.add(tf.keras.layers.SpatialDropout2D(0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filter_counts[1], (3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    INCLUDE_DROPOUT and model.add(tf.keras.layers.SpatialDropout2D(0.15))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filter_counts[2], (3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    INCLUDE_DROPOUT and model.add(tf.keras.layers.SpatialDropout2D(0.07))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(500, activation='relu'))\n",
    "    INCLUDE_DROPOUT and model.add(tf.keras.layers.Dropout(0.1))\n",
    "    model.add(tf.keras.layers.Dense(250, activation='relu'))\n",
    "    INCLUDE_DROPOUT and model.add(tf.keras.layers.Dropout(0.07))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_config3_model_part1(input_shape, n_classes, filter_counts, INCLUDE_DROPOUT):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filter_counts[0], (5, 5), input_shape=input_shape,\n",
    "                                     activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    INCLUDE_DROPOUT and model.add(tf.keras.layers.SpatialDropout2D(0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filter_counts[1], (3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    INCLUDE_DROPOUT and model.add(tf.keras.layers.SpatialDropout2D(0.15))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filter_counts[2], (3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    INCLUDE_DROPOUT and model.add(tf.keras.layers.SpatialDropout2D(0.1))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filter_counts[3], (3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    INCLUDE_DROPOUT and model.add(tf.keras.layers.SpatialDropout2D(0.07))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(500, activation='relu'))\n",
    "    INCLUDE_DROPOUT and model.add(tf.keras.layers.Dropout(0.1))\n",
    "    model.add(tf.keras.layers.Dense(250, activation='relu'))\n",
    "    INCLUDE_DROPOUT and model.add(tf.keras.layers.Dropout(0.07))\n",
    "    model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "    INCLUDE_DROPOUT and model.add(tf.keras.layers.Dropout(0.03))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_dynamic_diverse_data_generators():\n",
    "    zoom_range = np.random.choice([-0.2, -0.3, -0.1, 0.2])\n",
    "    shear_range = np.random.choice([0.2, 0.3, 0.1])\n",
    "    rotation_range = np.random.choice([0, 0.5, 0.4, 0.2, -0.2, -0.5, -0.4])\n",
    "    horizontal_flip = np.random.choice([True, False])\n",
    "    vertical_flip = np.random.choice([True, False])\n",
    "    featurewise_normalization = np.random.choice([True, False])\n",
    "    samplewise_normalization = np.random.choice([True, False])\n",
    "\n",
    "    train_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        scale=1.0/255.,\n",
    "        featurewise_std_normalization=featurewise_normalization,\n",
    "        samplewise_std_normalization=samplewise_normalization,\n",
    "        zoom_range=zoom_range,\n",
    "        shear_range=shear_range,\n",
    "        rotation_range=rotation_range,\n",
    "        horizontal_flip=horizontal_flip,\n",
    "        vertical_flip=vertical_flip)\n",
    "\n",
    "    validation_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        scale=1.0/255.,\n",
    "        featurewise_std_normalization=featurewise_normalization,\n",
    "        samplewise_std_normalization=samplewise_normalization,\n",
    "        zoom_range=zoom_range,\n",
    "        shear_range=shear_range,\n",
    "        rotation_range=rotation_range,\n",
    "        horizontal_flip=horizontal_flip,\n",
    "        vertical_flip=vertical_flip\n",
    "    )\n",
    "\n",
    "    train_generator = train_data_generator.flow(trainX, trainY, seed=random_seed)\n",
    "    validation_generator = validation_data_generator.flow(testX, testY, seed=random_seed)\n",
    "\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "\n",
    "def get_data_generators():\n",
    "    train_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        # featurewise_std_normalization=True, samplewise_std_normalization=True,\n",
    "        zoom_range=-0.2,\n",
    "        shear_range=0.2,\n",
    "        # rotation_range=0.5,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False)\n",
    "\n",
    "    validation_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        # featurewise_std_normalization=True, samplewise_std_normalization=True,\n",
    "        zoom_range=-0.2,\n",
    "        shear_range=0.2,\n",
    "        # rotation_range=0.5,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False\n",
    "    )\n",
    "\n",
    "    train_generator = train_data_generator.flow(trainX, trainY, seed=random_seed)\n",
    "    validation_generator = validation_data_generator.flow(testX, testY, seed=random_seed)\n",
    "\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "\n",
    "def main(trainX, trainY, testX, testY, configuration, save_flag, target_file, filter_counts,\n",
    "         INCLUDE_DROPOUT, USE_AUGMENTATION):\n",
    "    learning_rate = 0.01\n",
    "    n_epochs = 5\n",
    "    batch_size = 64\n",
    "\n",
    "    input_shape = trainX.shape[1:]\n",
    "\n",
    "    # number of unique classes, used to create N softmax layers\n",
    "    n_classes = np.unique(trainY).size\n",
    "\n",
    "    if configuration is 'config1':\n",
    "        model = get_config1_model_part1(input_shape, n_classes, filter_counts, INCLUDE_DROPOUT)\n",
    "    elif configuration is 'config2':\n",
    "        model = get_config2_model_part1(input_shape, n_classes, filter_counts, INCLUDE_DROPOUT)\n",
    "    elif configuration is 'config3':\n",
    "        model = get_config3_model_part1(input_shape, n_classes, filter_counts, INCLUDE_DROPOUT)\n",
    "    else:\n",
    "        # baseline model\n",
    "        model = get_default_model_part1(input_shape, n_classes, filter_counts, INCLUDE_DROPOUT)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
    "    # optimizer = tf.keras.optimizers.Adadelta(lr=learning_rate)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                  optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    if USE_AUGMENTATION:\n",
    "        # augmentation fit-generator\n",
    "        train_generator, validation_generator = get_data_generators()\n",
    "\n",
    "        train_steps = trainX.shape[0] // batch_size\n",
    "        validation_steps = testX.shape[0] // batch_size\n",
    "\n",
    "        history = model.fit_generator(train_generator, steps_per_epoch=train_steps,\n",
    "                                      epochs=n_epochs,\n",
    "                                      validation_data=validation_generator,\n",
    "                                      validation_steps=validation_steps)\n",
    "\n",
    "        plot_graph(history, n_epochs, save_flag, target_file)\n",
    "    else:\n",
    "        history = model.fit(trainX, trainY, validation_data=(testX, testY), batch_size=batch_size,\n",
    "                            epochs=n_epochs)\n",
    "\n",
    "        plot_graph(history, n_epochs, save_flag, target_file)\n",
    "\n",
    "\n",
    "def load_and_preprocess():\n",
    "    h5_file = './data1.h5'\n",
    "\n",
    "    trainX, trainY, testX, testY = loadDataH5(h5_file)\n",
    "\n",
    "    trainX = tf.keras.utils.normalize(trainX, axis=1)\n",
    "    testX = tf.keras.utils.normalize(testX, axis=1)\n",
    "\n",
    "    return trainX, trainY, testX, testY\n",
    "# configs = ['default']\n",
    "\n",
    "# for configuration in configs:\n",
    "#     target_file = './data/_' + configuration + '_plot_' + filters + '_deep_dense_'\n",
    "#     # target_file = \"./data/neagtive_zoom_with_shear_vertical_flip_\"\n",
    "# #     main(trainX, trainY, testX, testY, configuration, save_flag, target_file,\n",
    "# #          layers_filter_counts, INCLUDE_DROPOUT, USE_AUGMENTATION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def ensemble(trainX, trainY, testX, testY, save_flag, target_file,\n",
    "         filter_counts, INCLUDE_DROPOUT, USE_AUGMENTATION, n_epochs):\n",
    "    learning_rate = 0.01\n",
    "    n_epochs = n_epochs\n",
    "    batch_size = 64\n",
    "\n",
    "    # number of unique classes, used to create N softmax layers\n",
    "    n_classes = np.unique(trainY).size\n",
    "    \n",
    "    input_shape = trainX.shape[1:]\n",
    "    \n",
    "    filter_counts2 = [20, 40, 80, 160]\n",
    "    \n",
    "    # Diversity through different models and filter counts in each layers\n",
    "    model1 = get_default_model_part1(input_shape, n_classes, filter_counts, INCLUDE_DROPOUT)\n",
    "    model2 = get_config1_model_part1(input_shape, n_classes, filter_counts, INCLUDE_DROPOUT)\n",
    "    model3 = get_config2_model_part1(input_shape, n_classes, filter_counts, INCLUDE_DROPOUT)\n",
    "    model4 = get_config3_model_part1(input_shape, n_classes, filter_counts, INCLUDE_DROPOUT)\n",
    "    \n",
    "    model5 = get_default_model_part1(input_shape, n_classes, filter_counts2, False)\n",
    "    model6 = get_config1_model_part1(input_shape, n_classes, filter_counts2, False)\n",
    "    model7 = get_config2_model_part1(input_shape, n_classes, filter_counts2, False)\n",
    "    model8 = get_config3_model_part1(input_shape, n_classes, filter_counts2, False)\n",
    "    \n",
    "    models = [model1, model2, model3, model4, model5, model6, model7, model8]\n",
    "#     models = [model1]\n",
    "\n",
    "    train_steps = trainX.shape[0] // batch_size\n",
    "    validation_steps = testX.shape[0] // batch_size\n",
    "    \n",
    "    results_dict = dict()\n",
    "    accuracies_dict = dict()\n",
    "    \n",
    "    results = None\n",
    "    for model in models:\n",
    "        # Diversity in augmentation through dynamic setting changes in data generators \n",
    "        train_generator, validation_generator = get_data_generators()\n",
    "        \n",
    "        model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                     optimizer=tf.keras.optimizers.SGD(lr=learning_rate),\n",
    "                     metrics=['accuracy'])\n",
    "        \n",
    "        history = model.fit_generator(train_generator, steps_per_epoch=train_steps,\n",
    "                                      epochs=n_epochs)\n",
    "        results = model.predict(testX)\n",
    "    \n",
    "        results_dict[str(model)] = results\n",
    "        predictions = np.argmax(results, axis=1)\n",
    "        accuracy = accuracy_score(testY, predictions)\n",
    "        accuracies_dict[str(model)] = accuracy\n",
    "    \n",
    "    return results_dict, accuracies_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1020, 128, 128, 3) (1020,)\n",
      "(340, 128, 128, 3) (340,)\n",
      "Epoch 1/70\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 2.8328 - acc: 0.0618\n",
      "Epoch 2/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8126 - acc: 0.1108\n",
      "Epoch 3/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.7881 - acc: 0.1510\n",
      "Epoch 4/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.7505 - acc: 0.1598\n",
      "Epoch 5/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 2.7018 - acc: 0.2000\n",
      "Epoch 6/70\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 2.6369 - acc: 0.2167\n",
      "Epoch 7/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.5557 - acc: 0.2382\n",
      "Epoch 8/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.4640 - acc: 0.2500\n",
      "Epoch 9/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.3852 - acc: 0.2765\n",
      "Epoch 10/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.2949 - acc: 0.2833\n",
      "Epoch 11/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.2427 - acc: 0.2912\n",
      "Epoch 12/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.1706 - acc: 0.3059\n",
      "Epoch 13/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.1136 - acc: 0.3569\n",
      "Epoch 14/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.0622 - acc: 0.3549\n",
      "Epoch 15/70\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 2.0288 - acc: 0.3588\n",
      "Epoch 16/70\n",
      "32/32 [==============================] - 3s 89ms/step - loss: 1.9812 - acc: 0.3784\n",
      "Epoch 17/70\n",
      "32/32 [==============================] - 3s 97ms/step - loss: 1.9351 - acc: 0.4098\n",
      "Epoch 18/70\n",
      "32/32 [==============================] - 3s 90ms/step - loss: 1.9057 - acc: 0.3941\n",
      "Epoch 19/70\n",
      "32/32 [==============================] - 3s 96ms/step - loss: 1.8910 - acc: 0.4147\n",
      "Epoch 20/70\n",
      "32/32 [==============================] - 3s 89ms/step - loss: 1.8653 - acc: 0.4127\n",
      "Epoch 21/70\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 1.8371 - acc: 0.4078\n",
      "Epoch 22/70\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 1.8638 - acc: 0.4167\n",
      "Epoch 23/70\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 1.7876 - acc: 0.4333\n",
      "Epoch 24/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.7700 - acc: 0.4480\n",
      "Epoch 25/70\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 1.7486 - acc: 0.4549\n",
      "Epoch 26/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.7476 - acc: 0.4431\n",
      "Epoch 27/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.7207 - acc: 0.4549\n",
      "Epoch 28/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.6989 - acc: 0.4745\n",
      "Epoch 29/70\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 1.6727 - acc: 0.4755\n",
      "Epoch 30/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.6524 - acc: 0.4735\n",
      "Epoch 31/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.6454 - acc: 0.4882\n",
      "Epoch 32/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.6135 - acc: 0.4784\n",
      "Epoch 33/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.6013 - acc: 0.4990\n",
      "Epoch 34/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.5752 - acc: 0.4892\n",
      "Epoch 35/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.6067 - acc: 0.5039\n",
      "Epoch 36/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.5434 - acc: 0.5255\n",
      "Epoch 37/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.5570 - acc: 0.5088\n",
      "Epoch 38/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.5525 - acc: 0.5284\n",
      "Epoch 39/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.5339 - acc: 0.5275\n",
      "Epoch 40/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.4830 - acc: 0.5471\n",
      "Epoch 41/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.4874 - acc: 0.5392\n",
      "Epoch 42/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.4662 - acc: 0.5392\n",
      "Epoch 43/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.4649 - acc: 0.5333\n",
      "Epoch 44/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.4738 - acc: 0.5510\n",
      "Epoch 45/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.4490 - acc: 0.5647\n",
      "Epoch 46/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.4217 - acc: 0.5569\n",
      "Epoch 47/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.3911 - acc: 0.5706\n",
      "Epoch 48/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.3984 - acc: 0.5608\n",
      "Epoch 49/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.4043 - acc: 0.5569\n",
      "Epoch 50/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.3955 - acc: 0.5529\n",
      "Epoch 51/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.3346 - acc: 0.5863\n",
      "Epoch 52/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.3754 - acc: 0.5941\n",
      "Epoch 53/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.3408 - acc: 0.5824\n",
      "Epoch 54/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.3046 - acc: 0.5951: 0s - loss: 1.3016 - acc: 0.59\n",
      "Epoch 55/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.3134 - acc: 0.5853\n",
      "Epoch 56/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.3006 - acc: 0.5912\n",
      "Epoch 57/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.3010 - acc: 0.6000\n",
      "Epoch 58/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.2752 - acc: 0.6167\n",
      "Epoch 59/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.2661 - acc: 0.6069\n",
      "Epoch 60/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.2483 - acc: 0.6137\n",
      "Epoch 61/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.2595 - acc: 0.6176\n",
      "Epoch 62/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.2198 - acc: 0.6284\n",
      "Epoch 63/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.2184 - acc: 0.6402\n",
      "Epoch 64/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.2031 - acc: 0.6333\n",
      "Epoch 65/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.2130 - acc: 0.6343\n",
      "Epoch 66/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.1906 - acc: 0.6314\n",
      "Epoch 67/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.1953 - acc: 0.6441\n",
      "Epoch 68/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.1637 - acc: 0.6461\n",
      "Epoch 69/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.1817 - acc: 0.6304: 0s - loss: 1.1515 - a\n",
      "Epoch 70/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.1562 - acc: 0.6382\n",
      "Epoch 1/70\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 2.8336 - acc: 0.0618\n",
      "Epoch 2/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8285 - acc: 0.0657\n",
      "Epoch 3/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8225 - acc: 0.0627\n",
      "Epoch 4/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.8158 - acc: 0.0892\n",
      "Epoch 5/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.8056 - acc: 0.1069\n",
      "Epoch 6/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.7910 - acc: 0.1392\n",
      "Epoch 7/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.7713 - acc: 0.1510\n",
      "Epoch 8/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.7357 - acc: 0.1676\n",
      "Epoch 9/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.6851 - acc: 0.1833\n",
      "Epoch 10/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 2.6333 - acc: 0.1686\n",
      "Epoch 11/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.5616 - acc: 0.1863\n",
      "Epoch 12/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.5040 - acc: 0.2265\n",
      "Epoch 13/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.4123 - acc: 0.2441\n",
      "Epoch 14/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.3441 - acc: 0.2588\n",
      "Epoch 15/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 2.2626 - acc: 0.2784\n",
      "Epoch 16/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 74ms/step - loss: 2.2163 - acc: 0.2873\n",
      "Epoch 17/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.1479 - acc: 0.3020\n",
      "Epoch 18/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.0926 - acc: 0.3206\n",
      "Epoch 19/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.0207 - acc: 0.3461\n",
      "Epoch 20/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.9863 - acc: 0.3500\n",
      "Epoch 21/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.9504 - acc: 0.3608\n",
      "Epoch 22/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.8888 - acc: 0.3990\n",
      "Epoch 23/70\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 1.8542 - acc: 0.3892\n",
      "Epoch 24/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.8458 - acc: 0.3804\n",
      "Epoch 25/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.8134 - acc: 0.4137\n",
      "Epoch 26/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.7814 - acc: 0.4324\n",
      "Epoch 27/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.7649 - acc: 0.4343\n",
      "Epoch 28/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.7244 - acc: 0.4353\n",
      "Epoch 29/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.7124 - acc: 0.4343\n",
      "Epoch 30/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.6718 - acc: 0.4529\n",
      "Epoch 31/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.6580 - acc: 0.4755\n",
      "Epoch 32/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 1.6460 - acc: 0.4608\n",
      "Epoch 33/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.5713 - acc: 0.4863\n",
      "Epoch 34/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.6088 - acc: 0.4882\n",
      "Epoch 35/70\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 1.5859 - acc: 0.4873\n",
      "Epoch 36/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.5182 - acc: 0.5010\n",
      "Epoch 37/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.4981 - acc: 0.5206\n",
      "Epoch 38/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.5144 - acc: 0.5108\n",
      "Epoch 39/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.4850 - acc: 0.5108\n",
      "Epoch 40/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.4413 - acc: 0.5353\n",
      "Epoch 41/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.4604 - acc: 0.5216\n",
      "Epoch 42/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.4439 - acc: 0.5314\n",
      "Epoch 43/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.3972 - acc: 0.5471\n",
      "Epoch 44/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.3639 - acc: 0.5686\n",
      "Epoch 45/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.3457 - acc: 0.5637\n",
      "Epoch 46/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 1.3475 - acc: 0.5500\n",
      "Epoch 47/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.3209 - acc: 0.5578\n",
      "Epoch 48/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 1.2754 - acc: 0.5745\n",
      "Epoch 49/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.2514 - acc: 0.5912\n",
      "Epoch 50/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.2400 - acc: 0.6069\n",
      "Epoch 51/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.2640 - acc: 0.5961\n",
      "Epoch 52/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.2039 - acc: 0.5941\n",
      "Epoch 53/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 1.2150 - acc: 0.6127\n",
      "Epoch 54/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.1626 - acc: 0.6363\n",
      "Epoch 55/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 1.1341 - acc: 0.6127\n",
      "Epoch 56/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.1584 - acc: 0.6245\n",
      "Epoch 57/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.1254 - acc: 0.6245\n",
      "Epoch 58/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.0688 - acc: 0.6608\n",
      "Epoch 59/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.0557 - acc: 0.6324\n",
      "Epoch 60/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.0364 - acc: 0.6598\n",
      "Epoch 61/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.0338 - acc: 0.6559\n",
      "Epoch 62/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.0231 - acc: 0.6637\n",
      "Epoch 63/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 0.9624 - acc: 0.6784\n",
      "Epoch 64/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.9742 - acc: 0.6824\n",
      "Epoch 65/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.9579 - acc: 0.6971\n",
      "Epoch 66/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.9214 - acc: 0.6902\n",
      "Epoch 67/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.8973 - acc: 0.7088\n",
      "Epoch 68/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.9168 - acc: 0.7059\n",
      "Epoch 69/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.8699 - acc: 0.7216\n",
      "Epoch 70/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.8590 - acc: 0.7255\n",
      "Epoch 1/70\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 2.8336 - acc: 0.0559\n",
      "Epoch 2/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 2.8336 - acc: 0.0578\n",
      "Epoch 3/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8326 - acc: 0.0588\n",
      "Epoch 4/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8326 - acc: 0.0539\n",
      "Epoch 5/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8320 - acc: 0.0520\n",
      "Epoch 6/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 2.8311 - acc: 0.0716\n",
      "Epoch 7/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.8306 - acc: 0.0696: 1s - loss: 2.830\n",
      "Epoch 8/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.8297 - acc: 0.0745\n",
      "Epoch 9/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8288 - acc: 0.0931\n",
      "Epoch 10/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8277 - acc: 0.0814\n",
      "Epoch 11/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8273 - acc: 0.0912\n",
      "Epoch 12/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8257 - acc: 0.1039\n",
      "Epoch 13/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8243 - acc: 0.1000\n",
      "Epoch 14/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.8228 - acc: 0.1039\n",
      "Epoch 15/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.8210 - acc: 0.0990\n",
      "Epoch 16/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8189 - acc: 0.1127\n",
      "Epoch 17/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8151 - acc: 0.1186\n",
      "Epoch 18/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8114 - acc: 0.1020\n",
      "Epoch 19/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8031 - acc: 0.1353\n",
      "Epoch 20/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.7980 - acc: 0.1402\n",
      "Epoch 21/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.7850 - acc: 0.1422\n",
      "Epoch 22/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.7694 - acc: 0.1402\n",
      "Epoch 23/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.7390 - acc: 0.1451\n",
      "Epoch 24/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.7002 - acc: 0.1569\n",
      "Epoch 25/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.6502 - acc: 0.1529\n",
      "Epoch 26/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 2.5921 - acc: 0.1696\n",
      "Epoch 27/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.5191 - acc: 0.1794\n",
      "Epoch 28/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.4843 - acc: 0.1902\n",
      "Epoch 29/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.4229 - acc: 0.2069\n",
      "Epoch 30/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.4026 - acc: 0.1961\n",
      "Epoch 31/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.3525 - acc: 0.2206\n",
      "Epoch 32/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 76ms/step - loss: 2.3174 - acc: 0.2353\n",
      "Epoch 33/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.2793 - acc: 0.2392\n",
      "Epoch 34/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.2641 - acc: 0.2529\n",
      "Epoch 35/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.2305 - acc: 0.2569\n",
      "Epoch 36/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.1867 - acc: 0.2824\n",
      "Epoch 37/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 2.1585 - acc: 0.2873\n",
      "Epoch 38/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.1260 - acc: 0.2696\n",
      "Epoch 39/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.0834 - acc: 0.3176\n",
      "Epoch 40/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.0249 - acc: 0.3157\n",
      "Epoch 41/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.0100 - acc: 0.3284\n",
      "Epoch 42/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.9929 - acc: 0.3353\n",
      "Epoch 43/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.9603 - acc: 0.3549\n",
      "Epoch 44/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.9155 - acc: 0.3667\n",
      "Epoch 45/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.8853 - acc: 0.3745\n",
      "Epoch 46/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.8465 - acc: 0.3971\n",
      "Epoch 47/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.8154 - acc: 0.3912\n",
      "Epoch 48/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.8199 - acc: 0.3931\n",
      "Epoch 49/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.7684 - acc: 0.4127\n",
      "Epoch 50/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 1.7285 - acc: 0.4127\n",
      "Epoch 51/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.7398 - acc: 0.4000\n",
      "Epoch 52/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.6814 - acc: 0.4294\n",
      "Epoch 53/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.6762 - acc: 0.4451\n",
      "Epoch 54/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.6228 - acc: 0.4588\n",
      "Epoch 55/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.6110 - acc: 0.4667\n",
      "Epoch 56/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.5803 - acc: 0.4745\n",
      "Epoch 57/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.5714 - acc: 0.4667\n",
      "Epoch 58/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.5324 - acc: 0.4980\n",
      "Epoch 59/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.5332 - acc: 0.4716\n",
      "Epoch 60/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.5056 - acc: 0.4873\n",
      "Epoch 61/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.4840 - acc: 0.4863\n",
      "Epoch 62/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.4853 - acc: 0.4971\n",
      "Epoch 63/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.4403 - acc: 0.5010\n",
      "Epoch 64/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.4300 - acc: 0.5167\n",
      "Epoch 65/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.4129 - acc: 0.5333\n",
      "Epoch 66/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.3796 - acc: 0.5461\n",
      "Epoch 67/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.3682 - acc: 0.5549\n",
      "Epoch 68/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.3695 - acc: 0.5186\n",
      "Epoch 69/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.3227 - acc: 0.5657\n",
      "Epoch 70/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 1.2959 - acc: 0.5549\n",
      "Epoch 1/70\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 2.8332 - acc: 0.0578\n",
      "Epoch 2/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8332 - acc: 0.0608\n",
      "Epoch 3/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.8328 - acc: 0.0647\n",
      "Epoch 4/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8329 - acc: 0.0549\n",
      "Epoch 5/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.8326 - acc: 0.0598\n",
      "Epoch 6/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8323 - acc: 0.0608\n",
      "Epoch 7/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.8324 - acc: 0.0588\n",
      "Epoch 8/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8315 - acc: 0.0657\n",
      "Epoch 9/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8321 - acc: 0.0608\n",
      "Epoch 10/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8320 - acc: 0.0696\n",
      "Epoch 11/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8316 - acc: 0.0539\n",
      "Epoch 12/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.8314 - acc: 0.0569\n",
      "Epoch 13/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 2.8316 - acc: 0.0559\n",
      "Epoch 14/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8309 - acc: 0.0755\n",
      "Epoch 15/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8315 - acc: 0.0578\n",
      "Epoch 16/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8314 - acc: 0.0667\n",
      "Epoch 17/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8307 - acc: 0.0608\n",
      "Epoch 18/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8311 - acc: 0.0637\n",
      "Epoch 19/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.8310 - acc: 0.0598\n",
      "Epoch 20/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8303 - acc: 0.0667\n",
      "Epoch 21/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8298 - acc: 0.0735\n",
      "Epoch 22/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8303 - acc: 0.0588\n",
      "Epoch 23/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8302 - acc: 0.0686\n",
      "Epoch 24/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.8296 - acc: 0.0696\n",
      "Epoch 25/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8299 - acc: 0.0667\n",
      "Epoch 26/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.8295 - acc: 0.0598\n",
      "Epoch 27/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8295 - acc: 0.0775\n",
      "Epoch 28/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.8287 - acc: 0.0686\n",
      "Epoch 29/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8288 - acc: 0.0735\n",
      "Epoch 30/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8293 - acc: 0.0686\n",
      "Epoch 31/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8284 - acc: 0.0775\n",
      "Epoch 32/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8284 - acc: 0.0676\n",
      "Epoch 33/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8278 - acc: 0.0794\n",
      "Epoch 34/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.8275 - acc: 0.0735\n",
      "Epoch 35/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8274 - acc: 0.0735\n",
      "Epoch 36/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8263 - acc: 0.0824\n",
      "Epoch 37/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8264 - acc: 0.0775\n",
      "Epoch 38/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8264 - acc: 0.0725\n",
      "Epoch 39/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8243 - acc: 0.0833\n",
      "Epoch 40/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8245 - acc: 0.0794\n",
      "Epoch 41/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8251 - acc: 0.0843\n",
      "Epoch 42/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8241 - acc: 0.0725\n",
      "Epoch 43/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8232 - acc: 0.0775\n",
      "Epoch 44/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8221 - acc: 0.0912\n",
      "Epoch 45/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.8211 - acc: 0.0843\n",
      "Epoch 46/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8204 - acc: 0.0882\n",
      "Epoch 47/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8199 - acc: 0.0912\n",
      "Epoch 48/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8173 - acc: 0.0833\n",
      "Epoch 49/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8171 - acc: 0.0941\n",
      "Epoch 50/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.8139 - acc: 0.0912\n",
      "Epoch 51/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8122 - acc: 0.0971\n",
      "Epoch 52/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8121 - acc: 0.1225\n",
      "Epoch 53/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8071 - acc: 0.1059\n",
      "Epoch 54/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.8039 - acc: 0.1069\n",
      "Epoch 55/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 2.8012 - acc: 0.1029\n",
      "Epoch 56/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.7975 - acc: 0.1118\n",
      "Epoch 57/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.7931 - acc: 0.1039\n",
      "Epoch 58/70\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 2.7846 - acc: 0.1343\n",
      "Epoch 59/70\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 2.7744 - acc: 0.1216\n",
      "Epoch 60/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.7693 - acc: 0.1353\n",
      "Epoch 61/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.7530 - acc: 0.1353\n",
      "Epoch 62/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.7329 - acc: 0.1343\n",
      "Epoch 63/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.7048 - acc: 0.1549\n",
      "Epoch 64/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 2.6663 - acc: 0.1745\n",
      "Epoch 65/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.6154 - acc: 0.1657\n",
      "Epoch 66/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.5592 - acc: 0.1941\n",
      "Epoch 67/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.4903 - acc: 0.2049\n",
      "Epoch 68/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.4298 - acc: 0.2186\n",
      "Epoch 69/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.3375 - acc: 0.2451\n",
      "Epoch 70/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.2825 - acc: 0.2578\n",
      "Epoch 1/70\n",
      "32/32 [==============================] - 1s 47ms/step - loss: 2.8358 - acc: 0.0618\n",
      "Epoch 2/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 2.8056 - acc: 0.0971: 0s - loss: 2.8114 - \n",
      "Epoch 3/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.7714 - acc: 0.1147\n",
      "Epoch 4/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 2.7327 - acc: 0.1549\n",
      "Epoch 5/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.6743 - acc: 0.1833\n",
      "Epoch 6/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 2.6100 - acc: 0.2000\n",
      "Epoch 7/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.5441 - acc: 0.2108: 0s - loss: 2.5618 -\n",
      "Epoch 8/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 2.4658 - acc: 0.2490\n",
      "Epoch 9/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.3954 - acc: 0.2549\n",
      "Epoch 10/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 2.3314 - acc: 0.2814\n",
      "Epoch 11/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.2349 - acc: 0.3235\n",
      "Epoch 12/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 2.1621 - acc: 0.3225\n",
      "Epoch 13/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 2.1065 - acc: 0.3333: 1s - loss: 2.1\n",
      "Epoch 14/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 2.0563 - acc: 0.3569\n",
      "Epoch 15/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.0288 - acc: 0.3637\n",
      "Epoch 16/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.9796 - acc: 0.3824\n",
      "Epoch 17/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.9148 - acc: 0.4069\n",
      "Epoch 18/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.8893 - acc: 0.4020\n",
      "Epoch 19/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.8541 - acc: 0.4059\n",
      "Epoch 20/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.8395 - acc: 0.4010\n",
      "Epoch 21/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.7863 - acc: 0.4529\n",
      "Epoch 22/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.7913 - acc: 0.4353\n",
      "Epoch 23/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.7483 - acc: 0.4500\n",
      "Epoch 24/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.7299 - acc: 0.4343\n",
      "Epoch 25/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.7100 - acc: 0.4539\n",
      "Epoch 26/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.6638 - acc: 0.4931\n",
      "Epoch 27/70\n",
      "32/32 [==============================] - 2s 69ms/step - loss: 1.6662 - acc: 0.4745\n",
      "Epoch 28/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.6383 - acc: 0.4902\n",
      "Epoch 29/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.6185 - acc: 0.4843\n",
      "Epoch 30/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.5853 - acc: 0.5020\n",
      "Epoch 31/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.5802 - acc: 0.5010\n",
      "Epoch 32/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.5735 - acc: 0.5108: 0s - loss: 1.5415 - ac\n",
      "Epoch 33/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.5384 - acc: 0.5225: 0s - loss: 1.5583 - acc: 0\n",
      "Epoch 34/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.5470 - acc: 0.5059\n",
      "Epoch 35/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.4843 - acc: 0.5520\n",
      "Epoch 36/70\n",
      "32/32 [==============================] - 2s 69ms/step - loss: 1.4837 - acc: 0.5314\n",
      "Epoch 37/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.4774 - acc: 0.5461\n",
      "Epoch 38/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.5038 - acc: 0.5402\n",
      "Epoch 39/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.4139 - acc: 0.5549\n",
      "Epoch 40/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.4312 - acc: 0.5637\n",
      "Epoch 41/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.4305 - acc: 0.5647\n",
      "Epoch 42/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.4028 - acc: 0.5559: 1s - loss: 1.3855\n",
      "Epoch 43/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.3774 - acc: 0.5804\n",
      "Epoch 44/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.3564 - acc: 0.6078\n",
      "Epoch 45/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.3505 - acc: 0.5843\n",
      "Epoch 46/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.3551 - acc: 0.5804\n",
      "Epoch 47/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.3395 - acc: 0.5716\n",
      "Epoch 48/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.3303 - acc: 0.5882\n",
      "Epoch 49/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.2813 - acc: 0.6029\n",
      "Epoch 50/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.2740 - acc: 0.6235\n",
      "Epoch 51/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.2565 - acc: 0.6157\n",
      "Epoch 52/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.2560 - acc: 0.6157\n",
      "Epoch 53/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.2347 - acc: 0.6343\n",
      "Epoch 54/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.2403 - acc: 0.6235\n",
      "Epoch 55/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.2137 - acc: 0.6176\n",
      "Epoch 56/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.2163 - acc: 0.6206\n",
      "Epoch 57/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.2027 - acc: 0.6333\n",
      "Epoch 58/70\n",
      "32/32 [==============================] - 2s 69ms/step - loss: 1.1724 - acc: 0.6392\n",
      "Epoch 59/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.1798 - acc: 0.6392\n",
      "Epoch 60/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.1773 - acc: 0.6382\n",
      "Epoch 61/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.1405 - acc: 0.6510\n",
      "Epoch 62/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.1491 - acc: 0.6382\n",
      "Epoch 63/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 69ms/step - loss: 1.1205 - acc: 0.6441\n",
      "Epoch 64/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.1603 - acc: 0.6363\n",
      "Epoch 65/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.1180 - acc: 0.6608\n",
      "Epoch 66/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.0715 - acc: 0.6775\n",
      "Epoch 67/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.1147 - acc: 0.6461\n",
      "Epoch 68/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.0862 - acc: 0.6549\n",
      "Epoch 69/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.0488 - acc: 0.6804\n",
      "Epoch 70/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.0643 - acc: 0.6657\n",
      "Epoch 1/70\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 2.8325 - acc: 0.0500\n",
      "Epoch 2/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.8282 - acc: 0.0706\n",
      "Epoch 3/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.8226 - acc: 0.0882\n",
      "Epoch 4/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.8183 - acc: 0.0784\n",
      "Epoch 5/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.8107 - acc: 0.1255\n",
      "Epoch 6/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.8006 - acc: 0.1402\n",
      "Epoch 7/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.7881 - acc: 0.1431\n",
      "Epoch 8/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.7644 - acc: 0.1716\n",
      "Epoch 9/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.7317 - acc: 0.1765\n",
      "Epoch 10/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.6785 - acc: 0.1892\n",
      "Epoch 11/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.5983 - acc: 0.2059\n",
      "Epoch 12/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.5016 - acc: 0.2039\n",
      "Epoch 13/70\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 2.4003 - acc: 0.2225\n",
      "Epoch 14/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 2.2970 - acc: 0.2676\n",
      "Epoch 15/70\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 2.2122 - acc: 0.2873\n",
      "Epoch 16/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 2.1143 - acc: 0.3167\n",
      "Epoch 17/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.0493 - acc: 0.3353\n",
      "Epoch 18/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.9787 - acc: 0.3745\n",
      "Epoch 19/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.9197 - acc: 0.3755\n",
      "Epoch 20/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.8507 - acc: 0.4069\n",
      "Epoch 21/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.8115 - acc: 0.4000\n",
      "Epoch 22/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.7902 - acc: 0.4088\n",
      "Epoch 23/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.7382 - acc: 0.4314\n",
      "Epoch 24/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.7182 - acc: 0.4353\n",
      "Epoch 25/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.6759 - acc: 0.4382\n",
      "Epoch 26/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.6033 - acc: 0.4814\n",
      "Epoch 27/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.5817 - acc: 0.4716\n",
      "Epoch 28/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.5565 - acc: 0.4971\n",
      "Epoch 29/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.5574 - acc: 0.5029\n",
      "Epoch 30/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.5238 - acc: 0.5000\n",
      "Epoch 31/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.4943 - acc: 0.4794\n",
      "Epoch 32/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.4602 - acc: 0.5245\n",
      "Epoch 33/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.4194 - acc: 0.5314\n",
      "Epoch 34/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.4193 - acc: 0.5294\n",
      "Epoch 35/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.4060 - acc: 0.5480: 0s - loss: 1.3620 - \n",
      "Epoch 36/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.3497 - acc: 0.5676\n",
      "Epoch 37/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.3351 - acc: 0.5549\n",
      "Epoch 38/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.3147 - acc: 0.5627\n",
      "Epoch 39/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.2547 - acc: 0.6029\n",
      "Epoch 40/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.2974 - acc: 0.5608\n",
      "Epoch 41/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.2575 - acc: 0.5814\n",
      "Epoch 42/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.2118 - acc: 0.6108\n",
      "Epoch 43/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.2125 - acc: 0.6078\n",
      "Epoch 44/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.1651 - acc: 0.6314\n",
      "Epoch 45/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.1304 - acc: 0.6324\n",
      "Epoch 46/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.1227 - acc: 0.6490\n",
      "Epoch 47/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.1010 - acc: 0.6441\n",
      "Epoch 48/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.0791 - acc: 0.6392\n",
      "Epoch 49/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.0210 - acc: 0.6696\n",
      "Epoch 50/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.0698 - acc: 0.6637\n",
      "Epoch 51/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 0.9845 - acc: 0.6745: 1s - loss: 0.99\n",
      "Epoch 52/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 0.9452 - acc: 0.7049\n",
      "Epoch 53/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 0.9456 - acc: 0.6990\n",
      "Epoch 54/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 0.9408 - acc: 0.6990\n",
      "Epoch 55/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 0.9239 - acc: 0.7029\n",
      "Epoch 56/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 0.9148 - acc: 0.6990\n",
      "Epoch 57/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 0.8220 - acc: 0.7461\n",
      "Epoch 58/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 0.8082 - acc: 0.7431\n",
      "Epoch 59/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 0.8108 - acc: 0.7402\n",
      "Epoch 60/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 0.7804 - acc: 0.7441\n",
      "Epoch 61/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 0.7765 - acc: 0.7500\n",
      "Epoch 62/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 0.7368 - acc: 0.7637\n",
      "Epoch 63/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 0.6790 - acc: 0.7667\n",
      "Epoch 64/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 0.7152 - acc: 0.7696\n",
      "Epoch 65/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 0.6583 - acc: 0.7814\n",
      "Epoch 66/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 0.6323 - acc: 0.7922\n",
      "Epoch 67/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 0.6111 - acc: 0.8088\n",
      "Epoch 68/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 0.6034 - acc: 0.8098\n",
      "Epoch 69/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 0.5669 - acc: 0.8216\n",
      "Epoch 70/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 0.5210 - acc: 0.8304\n",
      "Epoch 1/70\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 2.8332 - acc: 0.0686\n",
      "Epoch 2/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8322 - acc: 0.0676\n",
      "Epoch 3/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.8308 - acc: 0.0735\n",
      "Epoch 4/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8299 - acc: 0.0794\n",
      "Epoch 5/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8286 - acc: 0.0804\n",
      "Epoch 6/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8274 - acc: 0.0961\n",
      "Epoch 7/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8264 - acc: 0.0951\n",
      "Epoch 8/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8247 - acc: 0.1059\n",
      "Epoch 9/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 72ms/step - loss: 2.8230 - acc: 0.1069\n",
      "Epoch 10/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8208 - acc: 0.1265\n",
      "Epoch 11/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.8189 - acc: 0.1069\n",
      "Epoch 12/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8155 - acc: 0.1304\n",
      "Epoch 13/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.8119 - acc: 0.1167\n",
      "Epoch 14/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.8070 - acc: 0.1353\n",
      "Epoch 15/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.8000 - acc: 0.1333\n",
      "Epoch 16/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.7893 - acc: 0.1588\n",
      "Epoch 17/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.7741 - acc: 0.1667\n",
      "Epoch 18/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.7536 - acc: 0.1716\n",
      "Epoch 19/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.7209 - acc: 0.1794\n",
      "Epoch 20/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.6682 - acc: 0.1902\n",
      "Epoch 21/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.5789 - acc: 0.1892\n",
      "Epoch 22/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.4859 - acc: 0.2039\n",
      "Epoch 23/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.3925 - acc: 0.2118\n",
      "Epoch 24/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.3301 - acc: 0.2225\n",
      "Epoch 25/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.2462 - acc: 0.2500\n",
      "Epoch 26/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.1732 - acc: 0.2647\n",
      "Epoch 27/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.1148 - acc: 0.3078\n",
      "Epoch 28/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.0672 - acc: 0.3196\n",
      "Epoch 29/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 2.0076 - acc: 0.3363\n",
      "Epoch 30/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.9453 - acc: 0.3510\n",
      "Epoch 31/70\n",
      "32/32 [==============================] - 2s 69ms/step - loss: 1.9186 - acc: 0.3578\n",
      "Epoch 32/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.8510 - acc: 0.3637\n",
      "Epoch 33/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.8038 - acc: 0.4000\n",
      "Epoch 34/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.7900 - acc: 0.3873\n",
      "Epoch 35/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.7254 - acc: 0.4078\n",
      "Epoch 36/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.7033 - acc: 0.4157\n",
      "Epoch 37/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.6643 - acc: 0.4353\n",
      "Epoch 38/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 1.6445 - acc: 0.4284\n",
      "Epoch 39/70\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 1.6039 - acc: 0.4667\n",
      "Epoch 40/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.5651 - acc: 0.4598\n",
      "Epoch 41/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 1.5685 - acc: 0.4716\n",
      "Epoch 42/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 1.5222 - acc: 0.5029\n",
      "Epoch 43/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.5275 - acc: 0.4784\n",
      "Epoch 44/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.4545 - acc: 0.4863\n",
      "Epoch 45/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.4434 - acc: 0.5049\n",
      "Epoch 46/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.4320 - acc: 0.5255\n",
      "Epoch 47/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.3892 - acc: 0.5373\n",
      "Epoch 48/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.3659 - acc: 0.5255\n",
      "Epoch 49/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.3406 - acc: 0.5490\n",
      "Epoch 50/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.3289 - acc: 0.5304\n",
      "Epoch 51/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.2993 - acc: 0.5657\n",
      "Epoch 52/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 1.2551 - acc: 0.5863\n",
      "Epoch 53/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.2809 - acc: 0.5559\n",
      "Epoch 54/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.1884 - acc: 0.5990\n",
      "Epoch 55/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.1829 - acc: 0.6098\n",
      "Epoch 56/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.1892 - acc: 0.5971\n",
      "Epoch 57/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.1092 - acc: 0.6324\n",
      "Epoch 58/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.2042 - acc: 0.6108\n",
      "Epoch 59/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.0580 - acc: 0.6402\n",
      "Epoch 60/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.0585 - acc: 0.6461\n",
      "Epoch 61/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.0003 - acc: 0.6833\n",
      "Epoch 62/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 0.9675 - acc: 0.6873\n",
      "Epoch 63/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 0.9771 - acc: 0.6794\n",
      "Epoch 64/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 0.9242 - acc: 0.6863\n",
      "Epoch 65/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 0.8524 - acc: 0.7049\n",
      "Epoch 66/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 0.8803 - acc: 0.6892\n",
      "Epoch 67/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 0.8352 - acc: 0.7176\n",
      "Epoch 68/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 0.8069 - acc: 0.7480\n",
      "Epoch 69/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 0.7775 - acc: 0.7667\n",
      "Epoch 70/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 0.7455 - acc: 0.7461\n",
      "Epoch 1/70\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 2.8329 - acc: 0.0637\n",
      "Epoch 2/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8322 - acc: 0.0627\n",
      "Epoch 3/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8316 - acc: 0.0627\n",
      "Epoch 4/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8312 - acc: 0.0627\n",
      "Epoch 5/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8306 - acc: 0.0627\n",
      "Epoch 6/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.8301 - acc: 0.0627\n",
      "Epoch 7/70\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 2.8299 - acc: 0.0627\n",
      "Epoch 8/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.8293 - acc: 0.0627\n",
      "Epoch 9/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8287 - acc: 0.0627\n",
      "Epoch 10/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.8285 - acc: 0.0627\n",
      "Epoch 11/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8278 - acc: 0.0627\n",
      "Epoch 12/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8275 - acc: 0.0618\n",
      "Epoch 13/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8268 - acc: 0.0608\n",
      "Epoch 14/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8264 - acc: 0.0647\n",
      "Epoch 15/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.8256 - acc: 0.0608\n",
      "Epoch 16/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8248 - acc: 0.0686\n",
      "Epoch 17/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8239 - acc: 0.0686\n",
      "Epoch 18/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8232 - acc: 0.0647\n",
      "Epoch 19/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.8220 - acc: 0.0647\n",
      "Epoch 20/70\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 2.8210 - acc: 0.0637\n",
      "Epoch 21/70\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 2.8194 - acc: 0.0657\n",
      "Epoch 22/70\n",
      "32/32 [==============================] - 3s 93ms/step - loss: 2.8184 - acc: 0.0637\n",
      "Epoch 23/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8165 - acc: 0.0657\n",
      "Epoch 24/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.8149 - acc: 0.0667\n",
      "Epoch 25/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 74ms/step - loss: 2.8127 - acc: 0.0647\n",
      "Epoch 26/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.8097 - acc: 0.0735\n",
      "Epoch 27/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.8063 - acc: 0.0667\n",
      "Epoch 28/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.8027 - acc: 0.0676\n",
      "Epoch 29/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.7969 - acc: 0.0667\n",
      "Epoch 30/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.7918 - acc: 0.0725\n",
      "Epoch 31/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.7829 - acc: 0.0755\n",
      "Epoch 32/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.7723 - acc: 0.0892\n",
      "Epoch 33/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.7570 - acc: 0.0931\n",
      "Epoch 34/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 2.7358 - acc: 0.1147\n",
      "Epoch 35/70\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 2.7066 - acc: 0.1333\n",
      "Epoch 36/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.6638 - acc: 0.1569\n",
      "Epoch 37/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.6032 - acc: 0.1931\n",
      "Epoch 38/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 2.5453 - acc: 0.1775\n",
      "Epoch 39/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.4742 - acc: 0.1912: 2s - loss: 2. - ETA: 0s - loss: 2.4784 - acc: \n",
      "Epoch 40/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.4245 - acc: 0.2069\n",
      "Epoch 41/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.3975 - acc: 0.2265\n",
      "Epoch 42/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 2.3039 - acc: 0.2451\n",
      "Epoch 43/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.2432 - acc: 0.2520\n",
      "Epoch 44/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.1847 - acc: 0.2706\n",
      "Epoch 45/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 2.1263 - acc: 0.2961\n",
      "Epoch 46/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 2.0849 - acc: 0.3108\n",
      "Epoch 47/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 2.0614 - acc: 0.3314\n",
      "Epoch 48/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.9840 - acc: 0.3431\n",
      "Epoch 49/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.9345 - acc: 0.3333\n",
      "Epoch 50/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.9300 - acc: 0.3480\n",
      "Epoch 51/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.8919 - acc: 0.3569\n",
      "Epoch 52/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.8173 - acc: 0.4098\n",
      "Epoch 53/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.8271 - acc: 0.3873\n",
      "Epoch 54/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.7970 - acc: 0.3902\n",
      "Epoch 55/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.7466 - acc: 0.4078\n",
      "Epoch 56/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.7177 - acc: 0.4176\n",
      "Epoch 57/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.7079 - acc: 0.4029\n",
      "Epoch 58/70\n",
      "32/32 [==============================] - 2s 71ms/step - loss: 1.6804 - acc: 0.4255\n",
      "Epoch 59/70\n",
      "32/32 [==============================] - 2s 70ms/step - loss: 1.6346 - acc: 0.4549\n",
      "Epoch 60/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.6598 - acc: 0.4480\n",
      "Epoch 61/70\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.5936 - acc: 0.4559\n",
      "Epoch 62/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.6034 - acc: 0.4549\n",
      "Epoch 63/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.5025 - acc: 0.4784: 0s - loss: 1.5084 - acc: 0.476\n",
      "Epoch 64/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.5335 - acc: 0.4794\n",
      "Epoch 65/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.4955 - acc: 0.4951\n",
      "Epoch 66/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.5349 - acc: 0.4843\n",
      "Epoch 67/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.4326 - acc: 0.5010\n",
      "Epoch 68/70\n",
      "32/32 [==============================] - 2s 72ms/step - loss: 1.4240 - acc: 0.5078\n",
      "Epoch 69/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.3615 - acc: 0.5275\n",
      "Epoch 70/70\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 1.4477 - acc: 0.5069\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "# random seed\n",
    "random_seed = 171450\n",
    "tf.random.set_random_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# uncomment only 1 of the below configurations to run it\n",
    "configuration = 'default'  # this is the baseline regression config\n",
    "# configuration = 'config1'\n",
    "# configuration = 'config2'\n",
    "# configuration = 'config3\n",
    "\n",
    "INCLUDE_DROPOUT = True\n",
    "USE_AUGMENTATION = True\n",
    "USE_REGULARIZATION = False\n",
    "save_flag = False\n",
    "\n",
    "trainX, trainY, testX, testY = load_and_preprocess()\n",
    "\n",
    "layers_filter_counts = [64, 128, 192, 256]\n",
    "filters = '_64_128_192_256_filters_'\n",
    "\n",
    "if INCLUDE_DROPOUT:\n",
    "    filters += 'dropout_'\n",
    "\n",
    "if USE_AUGMENTATION:\n",
    "    filters += '_best_augmentation_'\n",
    "\n",
    "if USE_REGULARIZATION:\n",
    "    filters += '_regularization_'\n",
    "\n",
    "# configs = ['default', 'config1', 'config2', 'config3']\n",
    "# for configuration in configs:\n",
    "\n",
    "target_file = './data/_plot_' + filters + '_deep_dense_'\n",
    "layers_filter_counts = [64, 128, 192, 256]\n",
    "# target_file = \"./data/neagtive_zoom_with_shear_vertical_flip_\"\n",
    "#     main(trainX, trainY, testX, testY, configuration, save_flag, target_file,\n",
    "#          layers_filter_counts, INCLUDE_DROPOUT, USE_AUGMENTATION)\n",
    "\n",
    "n_epochs = 70\n",
    "results_dict, accuracies_dict = ensemble(trainX, trainY, testX, testY, save_flag, target_file,\n",
    "     layers_filter_counts, INCLUDE_DROPOUT, USE_AUGMENTATION, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fc8a6c10a20>': 0.47941176470588237,\n",
       " '<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fc8a22cce48>': 0.48823529411764705,\n",
       " '<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fc8a6c2a358>': 0.4441176470588235,\n",
       " '<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fc8afc987f0>': 0.3058823529411765,\n",
       " '<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fc8af96e9e8>': 0.5029411764705882,\n",
       " '<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fc8af774080>': 0.48823529411764705,\n",
       " '<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fc8af6a2ba8>': 0.5264705882352941,\n",
       " '<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fc8af567048>': 0.4970588235294118}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.47941176470588237,\n",
       " 0.48823529411764705,\n",
       " 0.4441176470588235,\n",
       " 0.3058823529411765,\n",
       " 0.5029411764705882,\n",
       " 0.48823529411764705,\n",
       " 0.5264705882352941,\n",
       " 0.4970588235294118]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_weights = list(accuracies_dict.values())\n",
    "accuracy_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fc8a6c10a20>': array([[6.1752325e-01, 2.8743071e-03, 4.7901100e-03, ..., 7.8821685e-03,\n",
       "         6.5059727e-03, 2.2430817e-02],\n",
       "        [5.9010665e-04, 1.3494045e-02, 4.3836534e-02, ..., 2.2748115e-02,\n",
       "         1.5485631e-02, 4.0040936e-02],\n",
       "        [9.3725733e-02, 2.2204258e-03, 1.6242889e-03, ..., 3.7980359e-02,\n",
       "         7.9834340e-03, 1.9686177e-02],\n",
       "        ...,\n",
       "        [7.9771467e-05, 1.8451782e-03, 3.0400639e-02, ..., 1.4236759e-02,\n",
       "         6.2385984e-02, 7.2483765e-03],\n",
       "        [8.2698381e-03, 5.0346971e-02, 1.7369583e-02, ..., 9.0351552e-02,\n",
       "         1.7200175e-01, 9.7383559e-02],\n",
       "        [2.1202220e-01, 1.2983457e-04, 5.2033430e-03, ..., 1.2423936e-02,\n",
       "         3.5319889e-03, 2.7999375e-02]], dtype=float32),\n",
       " '<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fc8a22cce48>': array([[4.7232452e-01, 8.6454779e-04, 7.1741396e-04, ..., 3.5693832e-03,\n",
       "         2.6759563e-03, 4.9385108e-02],\n",
       "        [4.8167090e-05, 1.9600918e-03, 3.9098710e-03, ..., 2.3784889e-02,\n",
       "         1.7856619e-03, 6.4334407e-02],\n",
       "        [4.7297608e-02, 1.6915897e-03, 2.1896214e-04, ..., 2.0810246e-02,\n",
       "         1.9414963e-03, 3.2718249e-02],\n",
       "        ...,\n",
       "        [5.0970101e-07, 1.3270163e-03, 2.9695973e-02, ..., 6.5926015e-03,\n",
       "         7.9020664e-02, 3.6582586e-04],\n",
       "        [4.3616355e-03, 1.3176535e-02, 2.8183244e-03, ..., 1.7009160e-01,\n",
       "         4.2893499e-02, 1.1598391e-01],\n",
       "        [4.3856550e-02, 2.2878927e-05, 4.3610885e-04, ..., 8.9044971e-03,\n",
       "         2.1325320e-03, 1.0614209e-01]], dtype=float32),\n",
       " '<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fc8a6c2a358>': array([[6.1770409e-01, 1.8448129e-04, 4.7503501e-05, ..., 3.3619793e-04,\n",
       "         6.0596975e-04, 8.2908794e-02],\n",
       "        [9.3623414e-04, 5.9424043e-03, 2.7866259e-03, ..., 3.2797924e-03,\n",
       "         2.4852373e-03, 1.8496662e-01],\n",
       "        [2.6942950e-01, 4.5877718e-04, 8.2505145e-04, ..., 2.4281561e-02,\n",
       "         1.2128941e-03, 8.7822527e-02],\n",
       "        ...,\n",
       "        [1.2866470e-04, 5.6374366e-03, 1.6107740e-02, ..., 1.9480119e-03,\n",
       "         1.9700725e-01, 4.8949738e-04],\n",
       "        [1.0885937e-02, 2.0563737e-02, 8.7855784e-03, ..., 9.4628379e-02,\n",
       "         4.0539000e-02, 4.5009154e-01],\n",
       "        [1.3377494e-01, 5.4492986e-05, 2.4758591e-04, ..., 4.8075835e-03,\n",
       "         8.8006788e-04, 8.1165105e-02]], dtype=float32),\n",
       " '<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fc8afc987f0>': array([[0.07269751, 0.01870909, 0.00862798, ..., 0.01669742, 0.03215271,\n",
       "         0.1478484 ],\n",
       "        [0.01053756, 0.06382182, 0.04066738, ..., 0.04089188, 0.06360054,\n",
       "         0.09846821],\n",
       "        [0.14296201, 0.01752941, 0.01315149, ..., 0.02236383, 0.03667292,\n",
       "         0.05967169],\n",
       "        ...,\n",
       "        [0.0010113 , 0.05130837, 0.11171855, ..., 0.02251691, 0.07295404,\n",
       "         0.01029046],\n",
       "        [0.02034957, 0.14010616, 0.12059902, ..., 0.07574187, 0.13041501,\n",
       "         0.02673583],\n",
       "        [0.11002085, 0.01604505, 0.01643309, ..., 0.03488585, 0.03027877,\n",
       "         0.04732456]], dtype=float32),\n",
       " '<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fc8af96e9e8>': array([[6.5577382e-01, 3.9358977e-03, 2.1704731e-03, ..., 4.1307299e-03,\n",
       "         4.5801261e-03, 2.3121169e-02],\n",
       "        [6.0980610e-04, 1.3730436e-02, 3.4595702e-02, ..., 1.3733611e-02,\n",
       "         7.8462819e-03, 1.5044339e-01],\n",
       "        [1.3834079e-01, 2.6417151e-03, 1.0802168e-03, ..., 3.6777694e-02,\n",
       "         4.7989767e-03, 6.1087329e-02],\n",
       "        ...,\n",
       "        [5.3800253e-05, 2.2563089e-03, 1.1586726e-02, ..., 1.5124866e-02,\n",
       "         8.1009880e-02, 8.6240871e-03],\n",
       "        [3.8559502e-03, 6.5909274e-02, 4.5024408e-03, ..., 4.4839457e-02,\n",
       "         6.6533692e-02, 3.7987643e-01],\n",
       "        [3.5840660e-01, 1.2756590e-04, 1.2282281e-03, ..., 1.4073684e-02,\n",
       "         1.6968511e-03, 9.4507940e-02]], dtype=float32),\n",
       " '<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fc8af774080>': array([[5.2052104e-01, 1.3446574e-04, 5.5965840e-05, ..., 8.0444617e-03,\n",
       "         2.0051126e-03, 4.0556245e-02],\n",
       "        [2.9697328e-06, 1.7051735e-03, 2.1392941e-03, ..., 2.1800259e-02,\n",
       "         1.9848386e-03, 1.0861528e-02],\n",
       "        [1.6280769e-01, 3.8574831e-04, 8.4915344e-05, ..., 3.7689084e-01,\n",
       "         2.8616339e-03, 1.9742826e-02],\n",
       "        ...,\n",
       "        [4.9493072e-09, 9.7152351e-06, 1.4682662e-03, ..., 1.4913041e-03,\n",
       "         1.8061679e-02, 6.6308089e-06],\n",
       "        [5.2140886e-04, 1.0158500e-02, 4.7546133e-04, ..., 1.1271404e-01,\n",
       "         7.0654362e-02, 7.1776748e-02],\n",
       "        [8.6185820e-02, 1.5692423e-06, 1.7461811e-04, ..., 1.6793380e-02,\n",
       "         1.0484753e-03, 6.2443756e-02]], dtype=float32),\n",
       " '<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fc8af6a2ba8>': array([[7.2044629e-01, 4.7007430e-05, 4.4963008e-06, ..., 1.0914525e-04,\n",
       "         3.1874562e-05, 3.0530643e-02],\n",
       "        [2.5739651e-05, 8.5336197e-04, 7.5700029e-04, ..., 1.4929766e-03,\n",
       "         1.3197082e-04, 1.4403987e-02],\n",
       "        [8.3957747e-02, 5.4246739e-05, 9.7829570e-06, ..., 2.0729201e-02,\n",
       "         1.1220158e-04, 3.8011681e-02],\n",
       "        ...,\n",
       "        [1.8502186e-07, 3.8251819e-04, 3.4147672e-02, ..., 5.3603016e-04,\n",
       "         4.8166133e-02, 5.1139482e-06],\n",
       "        [2.3840289e-03, 5.0642309e-03, 4.3073390e-04, ..., 5.0609894e-02,\n",
       "         2.5367537e-03, 3.6131375e-02],\n",
       "        [4.6414401e-02, 7.1058016e-06, 1.2362498e-05, ..., 7.1306003e-04,\n",
       "         7.4847339e-05, 6.6618748e-02]], dtype=float32),\n",
       " '<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fc8af567048>': array([[6.8510956e-01, 5.8170193e-04, 1.8846037e-04, ..., 2.1753483e-03,\n",
       "         1.5532012e-03, 5.5988643e-02],\n",
       "        [3.3723882e-03, 1.5276110e-02, 1.4972569e-02, ..., 1.1049845e-02,\n",
       "         5.6970306e-03, 6.3135967e-02],\n",
       "        [2.2212768e-01, 1.9536836e-03, 2.0668590e-04, ..., 1.6619952e-02,\n",
       "         1.3361123e-03, 6.2229503e-02],\n",
       "        ...,\n",
       "        [1.6729790e-04, 1.3309125e-02, 2.1817835e-02, ..., 3.3362748e-03,\n",
       "         5.5256702e-02, 2.9760430e-04],\n",
       "        [1.3114141e-02, 6.7866333e-02, 2.5428638e-02, ..., 2.3021512e-01,\n",
       "         4.0678672e-02, 2.8145574e-02],\n",
       "        [7.9483666e-02, 5.7608070e-04, 5.6366506e-04, ..., 2.3992389e-02,\n",
       "         3.5810610e-03, 5.9208240e-02]], dtype=float32)}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  6,  4, 16,  6,  7,  2,  1,  4,  2,  1,  2, 14, 11, 13,  8, 15,\n",
       "       14, 15,  0,  5, 11,  0,  5, 13,  1,  4,  0, 14, 11, 10,  4, 13, 10,\n",
       "       12,  5,  5, 10,  8,  7, 11, 16,  4,  6,  8, 11, 14,  0,  0, 11,  5,\n",
       "       13, 14,  4,  5, 16, 13,  6, 15, 16,  8, 15, 13,  8,  1,  4, 14,  8,\n",
       "        1,  5, 16,  8,  7,  0, 11,  3, 14, 16,  0, 11, 11,  4,  1,  5,  2,\n",
       "        5, 12, 11,  5,  8, 12,  4,  5, 14,  2, 16,  8,  1, 12,  6, 13,  6,\n",
       "        4, 11,  6,  8, 13,  3,  7,  7,  4,  0, 16,  6, 15,  7, 14,  0, 16,\n",
       "        3, 11, 13,  8,  8,  8, 14,  5, 14,  8, 12,  7,  0, 15,  7,  1,  8,\n",
       "       11, 16, 14,  4,  0,  1, 13,  8,  5,  8,  5, 14,  1,  5, 13, 14,  7,\n",
       "       11,  7, 14, 16,  7, 16,  2,  7, 11,  8, 16, 12, 13, 13,  2, 14, 11,\n",
       "       10, 13,  9, 15, 12, 11, 12,  8,  0,  6, 15,  8, 11, 15, 14,  8, 11,\n",
       "        6, 10,  6,  9,  8,  0,  8,  3, 14,  2,  5, 14, 12, 14, 16, 16,  2,\n",
       "        6,  4,  7,  0,  8,  6,  5,  1,  7,  0,  4, 14,  9,  6,  4,  2, 12,\n",
       "        5,  7, 13, 11,  4,  2,  1, 13,  0,  5,  5,  0,  3, 12,  6,  8, 13,\n",
       "        6,  7, 12,  0, 12, 10,  0, 14,  7,  4,  6,  2, 13, 11,  9, 16,  4,\n",
       "       13, 11, 11,  1,  1, 11, 14,  5,  6,  6,  8,  8, 14,  8, 13,  9,  8,\n",
       "        1, 12, 14, 14,  0,  9,  1,  8, 16, 11,  1, 13, 11,  8, 14,  3, 11,\n",
       "        7, 10,  4,  5, 12, 12,  4,  6,  5, 16, 12,  4, 15,  0,  5,  8, 13,\n",
       "       13,  3, 13,  7,  6,  3,  7,  8, 16,  7,  7,  3,  7, 14,  4,  1,  3,\n",
       "       10,  3, 15,  1, 11,  8,  9,  5, 16,  9,  9, 14,  5,  9,  5,  9,  4])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted = False\n",
    "\n",
    "all_results = np.array(list(results_dict.values()))\n",
    "\n",
    "if weighted:\n",
    "    final_result = np.average(all_results, axis=0, weights=np.array(accuracy_weights))\n",
    "else:\n",
    "    final_result = np.average(all_results, axis=0)    \n",
    "\n",
    "final_predictions = final_result.argmax(axis=1)\n",
    "final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.68      0.63        19\n",
      "           1       0.42      0.50      0.46        16\n",
      "           2       0.75      0.45      0.56        20\n",
      "           3       0.55      0.30      0.39        20\n",
      "           4       0.30      0.44      0.36        16\n",
      "           5       0.30      0.44      0.36        18\n",
      "           6       0.76      0.89      0.82        18\n",
      "           7       0.57      0.62      0.59        21\n",
      "           8       0.53      0.67      0.59        27\n",
      "           9       0.91      0.45      0.61        22\n",
      "          10       0.38      0.13      0.19        23\n",
      "          11       0.68      0.83      0.75        23\n",
      "          12       0.41      0.35      0.38        20\n",
      "          13       0.92      0.96      0.94        23\n",
      "          14       0.34      0.53      0.42        19\n",
      "          15       0.09      0.07      0.08        15\n",
      "          16       0.50      0.50      0.50        20\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       340\n",
      "   macro avg       0.53      0.52      0.51       340\n",
      "weighted avg       0.55      0.53      0.52       340\n",
      "\n",
      "0.5294117647058824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(classification_report(testY, final_predictions))\n",
    "accuracy = accuracy_score(testY, final_predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc8acdf84a8>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_improvements = np.array([accuracy]) - np.array(accuracy_weights)\n",
    "print(len(accuracy_improvements))\n",
    "\n",
    "model_labels = list()\n",
    "for model in range(1, 9):\n",
    "    model_title = 'Model - ' + str(model)\n",
    "    model_labels.append(model_title)\n",
    "\n",
    "improvements_df = pd.DataFrame(columns=model_labels)\n",
    "improvements_df = improvements_df.append(pd.Series(accuracy_improvements, index=model_labels), ignore_index=True)\n",
    "improvements_df = improvements_df.T\n",
    "improvements_df.plot(title='Improvement over other models', kind='barh', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model - 1</th>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model - 2</th>\n",
       "      <td>0.041176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model - 3</th>\n",
       "      <td>0.085294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model - 4</th>\n",
       "      <td>0.223529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model - 5</th>\n",
       "      <td>0.026471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model - 6</th>\n",
       "      <td>0.041176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model - 7</th>\n",
       "      <td>0.002941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model - 8</th>\n",
       "      <td>0.032353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "Model - 1  0.050000\n",
       "Model - 2  0.041176\n",
       "Model - 3  0.085294\n",
       "Model - 4  0.223529\n",
       "Model - 5  0.026471\n",
       "Model - 6  0.041176\n",
       "Model - 7  0.002941\n",
       "Model - 8  0.032353"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO add snapshot of this df to the report - Part A, task iii\n",
    "improvements_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
